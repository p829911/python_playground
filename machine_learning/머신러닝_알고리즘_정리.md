# 머신러닝 알고리즘 장단점 정리

### KNN

- 매개변수
  - 데이터 포인트 사이의 거리를 재는 방법 (기본적으로 유클리디안)
  - 이웃의 수 (3개 ~ 5개 정도로 적을 때 잘 작동)
- 장단점
  - 이해하기 매우 쉬운 모델, 많이 조정하지 않아도 자주 좋은 성능을 발휘
  - 매우 빠르게 만들 수 있지만, 훈련 세트가 매우 크면 (특성의 수나 샘플의 수가 클 경우) 예측이 느려진다.
  - 수백 개 이상의 많은 특성을 가진 데이터셋에는 잘 동작하지 않으며,
  - 특성 값 대부분이 0인 데이터셋과는 특히 잘 작동하지 않는다.
- 정리
  - `KNN` 은 이해하기 쉽지만, 예측이 느리고 많은 특성을 처리하는 능력이 부족해 현업에서는 잘 쓰지 않는다.



### 선형모델

- 회귀 선형 모델
  - 최소제곱법 (`OLS`, ordinary least squares)
  - 리지 (`Ridge`)
  - 라쏘 (`Lasso`)
  - 리지 + 라쏘 (`ElasticNet`)
- 분류용 선형 모델
  - 로지스틱회귀 (`LogisticRegression`)
  - SVM (`LinearSVC`)
- 매개변수
  - 회귀모델 (`alpha`)
  - `LinearSVC`, `LogisticRegression` (`C`)
  - `alpha` 값이 클수록, `C` 값이 작을수록 모델이 단순해 진다. (규제의 정도가 커짐)
  - 보통 `C`와 `alpha`는 로그 스케일로 최적치를 정한다.
  - 그리고 L1 규제를 사용할지 L2 규제를 사용할지를 정해야 한다.
  - 중요한 특성이 많지 않다고 생각하면 L1 규제를 사용, 그렇지 않으면 기본적으로 L2 규제를 사용
- 장단점
  - 학습 속도가 빠르고 예측도 빠르다. 매우 큰 데이터셋과 희소한 데이터셋에도 잘 작동한다.
  - 회귀와 분류에서 공식을 사용해 예측이 어떻게 만들어지는지 비교적 쉽게 이해할 수 있다.
  - 하지만 계수의 값들이 왜 그런지 명확하지 않을 때가 종종 있다. 특히 데이터셋의 특성들이 서로 깊게 연관되어 있을 때 그렇다.
  - 선형 모델은 샘플에 비해 특성이 많을 때 잘 작동한다. 다른 모델로 학습하기 어려운 매우 큰 데이터셋에도 선형 모델을 많이 사용한다. 그러나 저 차원의 데이터셋에서는 다른 모델들의 일반화 성능이 좋다. 
  - 커널 서포트 벡터 머신에서 선형 모델이 실패하는 예를 볼 수 있다.



### 나이브 베이즈 분류기

- `LogisticRegression`이나 `LinearSVC` 같은 선형 분류기보다 훈련 속도가 빠른 편이지만, 그 대신 일반화 성능이 조금 뒤진다.
- 나이브 베이즈 분류기가 효과적인 이유는 각 특성을 개별로 취급해 파라미터를 학습하고 각 특성에서 클래스별 통계를 단순하게 취합하기 때문이다.
- `GaussianNB` (연속적 데이터) , `BernoulliNB` (이진 데이터), `MultinomialNB` (카운트 데이터)
- 매개변수
  - `MultinomialNB` 와 `BernoulliNB` 는 모델의 복잡도를 조절하는 `alpha` 매개변수 하나를 가지고 있다. `alpha` 가 주어지면 알고리즘이 모든 특성에 양의 값을 가진 가상의 데이터 포인트를 `alpha` 개수만큼 추가한다.
  - `GaussianNB` 는 대부분 매우 고차원적인 데이터셋에 사용하고, 다른 두 나이브 베이즈 모델은 텍스트 같은 희소한 데이터를 카운트하는데 사용한다. 
- 장단점
  - 나이브 베이즈 모델과 선형 모델의 장단점은 비슷하다. 훈련과 예측 속도가 빠르며 훈련 과정을 이해하기 쉽다. 희소한 고차원 데이터에서 잘 작동하며 비교적 매개변수에 민감하지 않다.
  - 선형 모델로는 학습 시간이 너무 오래 걸리는 매우 큰 데이터셋에는 나이브 베이즈 모델을 시도해볼 만하며 종종 사용된다.



### 결정 트리

- 매개변수
  - `max_depth`, `max_leaf_nodes`, `min_samples_leaf` 중 하나만 지정해도 과대적합을 막는 데 충분하다. 
- 장단점
  - 만들어진 모델을 쉽게 시각화 할 수 있어서 비 전문가도 이해하기 쉽다 (비교적 작은 트리일 때)
  - 데이터 스케일에 구애받지 않는다.
  - 각 특성이 개별적으로 처리되어 데이터를 분할하는데 데이터 스케일의 영향을 받지 않으므로 결정 트리에서는 특성의 정규화나 표준화 같은 전처리 과정이 필요 없다.
  - 특히 특성의 스케일이 서로 다르거나 이진 특성과 연속적인 특성이 혼합되어 있을 때도 잘 작동한다.
  - 결정 트리의 주요 단점은 사전 가지치기를 사용함에도 불구하고 과대적합되는 경향이 있어 일반화 성능이 좋지 않다는 것이다.



### 결정 트리의 앙상블

- 랜덤 포레스트
  - 성능이 매우 뛰어나고 매개변수 튜닝을 많이 하지 않아도 잘 작동하며 데이터의 스케일을 맞출 필요도 없다.
  - 기본적으로 랜덤 포레스트는 단일 트리의 단점을 보완하고 장점은 그대로 가지고 있다.
  - 만약 의사 결정 과정을 간소하게 표현해야 한다면 단일 트리를 사용할 수 있다. 왜냐하면 수십, 수백 개의 트리를 자세히 분석하기 어렵고 랜덤 포레스트의 트리는 (특성의 일부만 사용하므로) 결정 트리보다 더 깊어지는 경향도 있기 때문이다. 그러므로 비 전문가에게 예측 과정을 시각적으로 보여주기 위해서는 하나의 결정트리가 더 좋은 선택이다. 
  - 랜덤 포레스트는 이름 그대로 랜덤하다. 그래서 다른 `random_state` 를 지정하면 전혀 다른 모델이 만들어 진다. 랜덤 포레스트의 트리가 많을수록 `random_state` 값의 변화에 따른 변동이 적다.
- 매개변수
  - `n_estimators` , `max_features` , `max_depth` 사전 가지치기 옵션
  - `n_estimators` 는 클수록 좋다. 더 많은 트리를 평균하면 과대적합을 줄여 더 안정적인 모델을 만든다.
  - 더 많은 트리는 더 많은 메모리와 긴 훈련 시간으로 이어진다. "가용한 시간과 메모리만큼 많이" 만드는 것이 좋다.
  - `max_features` 는 각 트리가 얼마나 무작위가 될지를 결정하며 작은 `max_feature` 는 과대적합을 줄여준다. 일반적으로 기본값을 쓰는 것이 좋다.
- 장단점
  - 랜덤 포레스트는 텍스트 데이터 같이 매우 차원이 높고 희소한 데이터에는 잘 작동하지 않는다. 이런 데이터는 선형 모델이 더 적합하다.
  - 랜덤 포레스트는 매우 큰 데이터셋에도 잘 작동하며 훈련은 여러 CPU 코어로 간단하게 병렬화 할 수 있다.
  - 랜덤 포레스트는 선형 모델보다 많은 메모리를 사용하며 훈련과 예측이 느리다.



- 그래디언트 부스팅 회귀 트리
  - 랜덤 포레스트와는 달리 그래디언트 부스팅은 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만든다.
  - 기본적으로 그래디언트 부스팅 회귀 트리에는 무작위성이 없다. 대신 강력한 사전 가지치기가 사용된다.
  - 그래디언트 부스팅 트리는 보통 하나에서 다섯 정도의 깊지 않은 트리를 사용하므로 메모리를 적게 사용하고 예측도 빠르다.
  - 이전 트리의 오차를 얼마나 강하게 보정할 것인지를 제어하는 `learning rate` 매개변수가 중요하다.
- 매개변수
  - `n_estimators` , `learning_rate`
  - 이 두 매개변수는 매우 깊게 연관되며 `learning_rate` 를 낮추면 비슷한 복잡도의 모델을 만들기 위해서 더 많은 트리를 추가해야 한다. `n_estimators` 가 클수록 좋은 랜덤 포레스트와는 달리 그래디언트 부스팅에서 `n_estimators` 를 크게하면 모델이 복잡해지고 과대적합될 가능성이 높아진다.
  - 일반적인 관례는 가용한 시간과 메모리 한도에서 `n_estimators` 를 맞추고 나서 적절한 `learning rate` 를 찾는 것이다.
  - 중요한 또 다른 매개변수는 각 트리의 복잡도를 낮추는 `max_depth` 또는 `max_leaf_node` 이다. 통상 그래디언트 부스팅 모델에서는 `max_depth` 를 매우 작게 설정하며 트리의 깊이가 5보다 깊어지지 않게 한다.
- 장단점
  - 그래디언트 부스팅 결정 트리는 지도 학습에서 가장 강력하고 널리 사용되는 모델 중 하나이다.
  - 가장 큰 단점은 매개변수를 잘 조정해야 한다는 것과 훈련 시간이 길다는 것이다.
  - 다른 트리기반 모델 처럼 특성의 스케일을 조정하지 않아도 되고 이진 특성이나 연속적인 특성에서도 잘 동작한다.
  - 트리 기반 모델의 특성상 희소한 고차원 데이터에는 잘 작동하지 않는다.



### 커널 서포트 벡터 머신

- 매개변수
  - 규제 매개변수 `C`이고 어떤 커널을 사용할지와 각 커널에 따른 매개변수이다.
  - RBF 커널은 가우시안 커널 폭의 역수인 `gamma` 매개변수를 갖는다.
  - `gamma` 와 `C` 모두 모델의 복잡도를 조정하며 둘 다 큰 값이 더 복잡한 모델을 만든다.
  - 그러므로 연관성이 많은 이 두 매개변수를 잘 설정하려면 `C` 와 `gamma` 를 함께 조정해야 한다.
- 장단점
  - SVM은 데이터의 특성이 몇 개 안 되더라도 복잡한 결정 경계를 만들 수 있다.
  - 저차원과 고차원의 데이터에 모두 잘 작동하지만 샘플이 많을 때는 잘 맞지 않는다.
  - 데이터 전처리와 매개변수 설정에 신경을 많이 써야 한다.
  - 분석하기도 어렵다. 예측이 어떻게 결정되었는지 이해하기 어렵고 비 전문가에게 모델을 설명하기가 난해하다.



### 신경망(딥러닝)

- 매개변수
  - 은닉층의 개수와 각 은닉층의 유닛 수이다.
  - 처음에는 한 개 또는 두 개의 은닉층으로 시작해서 늘려가야 한다.
  - 각 은닉층의 유닛 수는 보통 입력 특성의 수와 비슷하게 설정하지만 수천 초중반을 넘는 일은 거의 없다.
  - 신경망의 매개변수를 조정하는 일반적인 방법은 먼저 충분히 과대적합되어서 문제를 해결할 만한 큰 모델을 만든다. 그런 다음 훈련 데이터가 충분히 학습될 수 있다고 생각될 때 신경망 구조를 줄이거나 규제 강화를 위해 alpha 값을 증가시켜 일반화 성능을 향상시킨다.
  - `solver` 매개변수를 사용해 모델을 학습시키는 방법 또는 매개변수 학습에 사용하는 알고리즘을 지정할 수 있다.  (`adam` , `lbfgs` , `sgd`)
- 장단점
  - 대량의 데이터에 내재된 정보를 잡아내고 매우 복잡한 모델을 만들 수 있다.
  - 충분한 연산 시간과 데이터를 주고 매개변수를 세심하게 조정하면 신경망은 종종 다른 머신러닝 알고리즘을 뛰어넘는 성능을 낸다.
  - 신경망은 (특히 크고 강력한 모델이라면) 종종 학습이 오래 걸린다.
  - 데이터 전처리에 주의해야 한다.
  - SVM과 비슷하게 모든 특성이 같은 의미를 가진 동질의 데이터에서 잘 작동한다.
  - 다른 종류의 특성을 가진 데이터라면 트리 기반 모델이 더 잘 작동할 수 있다.