## 비지도 학습과 데이터 전처리

비지도 학습이란 알고 있는 출력값이나 정보 없이 학습 알고리즘을 가르쳐야 하는 모든 종류의 머신러닝을 가리킨다.

### 비지도 학습의 종류

- 비지도 변환 (unsupervised transformation)
  - 비지도 변환은 데이터를 새롭게 표현하여 사람이나 다른 머신러닝 알고리즘이 원래 데이터보다 쉽게 해석할 수 있도록 만드는 알고리즘이다. 비지도 변환이 널리 사용되는 분야는 특성이 많은 고차원 데이터를 특성의 수를 줄이면서 꼭 필요한 특징을 포함한 데이터로 표현하는 방법인 차원 축소 (dimensionality reduction)이다. 차원 축소의 대표적인 예는 시각화를 위해 데이터셋을 2차원으로 변경하는 경우이다.
- 군집 (clustering)
  - 데이터를 비슷한 것끼리 그룹으로 묶는 것이다.



### 비지도 학습의 도전 과제

- 비지도 학습에서 가장 어려운 일은 알고리즘이 뭔가 유용한 것을 학습했는지 평가하는 것이다. 

- 알고리즘에게 우리가 원하는 것을 알려줄 방법이 없다.
- 데이터 과학자가 데이터를 더 잘 이해하고 싶을 때 탐색적 분석 단계에서 많이 사용한다.
- 지도 학습의 전처리 단계에서도 사용한다.



### 데이터 전처리와 스케일 조정

<img src="https://user-images.githubusercontent.com/17154958/54887995-58bc0700-4edc-11e9-8a6c-9c1d24a0aa94.png" width=500/>

- 왼쪽 그래프는 두 개의 특성을 인위적으로 만든 이진 분류 데이터 셋이다. 첫 번째 특성 (x 축) 은 10과 15사이에 있다. 두 번째 특성 (y 축) 은 1과 9 사이에 있다.

- scikit-learn의 `StandardScaler` 는 각 특성의 평균을 0, 분산을 1로 변경하여 모든 특성이 같은 크기를 가지게 한다.
  $$
  \text{표준 점수, 표준값, z-점수} = \dfrac{x - \bar{x}}{\sigma} \;\;(\bar{x}: 평균, \;\;\sigma : \text{표준편차})
  $$
  그러나 이 방법은 특성의 최솟값과 최댓값 크기를 제한하지 않는다.

- `RobustScaler` 는 특성들이 같은 스케일을 갖게 된다는 통계적 측면에서는 `StandardScaler` 와 비슷하다. 하지만 평균과 분산 대신 중간 값 (median) 과 사분위 값 (quartile) 을 사용한다.
  $$
  \text{RobustScaler} = \dfrac{x - q_2}{q_3 - q_1} \;\;(q_2: \text{중간값}, \; q_3: \text{3사분위 값}, \; q_1: \text{1사분위 값})
  $$
  이런 방식 떄문에 `RobustScaler` 는 전체 데이터와 아주 동떨어진 데이터 포인트에 영향을 받지 않는다. 이런 이상 데이터를 이상치 (outler) 라 하며 다른 스케일 조정 기법에서는 문제가 될 수 있다.

- `MinMaxScaler` 는 모든 특성이 정확하게 0과 1 사이에 위치하도록 데이터를 변경한다.
  $$
  \text{MinMaxScaler} = \dfrac{x - x_{min}}{x_{max} - x_{min}}
  $$

- `Normalizer` 는 매우 다른 스케일 조정 기법이다. 이 방식은 특성 벡터의 유클리디안 길이가 1이 되도록 데이터 포인트를 조정한다.

  - `Normalizer` 의 `norm` 매개변수는 `l1` , `l2` , `max` 의 세 가지 옵션을 제공하며 유클리디안 거리를 의미하는 `l2` 가 기본값이다.
  - `StandardScaler` , `RobustScaler` , `MinMaxScaler` 는 각 열 (특성) 의 통계치를 이용하지만 `Normalizer` 는 행 (데이터 포인트) 마다 각기 정규화된다.

  다른 말로 하면 지름이 1인 원 (3차원 일 땐 구) 에 데이터 포인트를 투영한다. 이러한 정규화는 특성 벡터의 길이는 상관 없고 데이터의 방향만이 중요할 때 많이 사용한다.



### 차원 축소, 특성 추출, 매니폴드 학습

#### 주성분 분석 (PCA)

- 주성분 분석은 특성들이 통계적으로 상관관계가 없도록 데이터셋을 회전시키는 기술이다.

  회전한 뒤에 데이터를 설명하는 데 얼마나 중요하냐에 따라 종종 새로운 특성 중 일부만 선택된다.

<img src="https://user-images.githubusercontent.com/17154958/54888975-3c709800-4ee5-11e9-8e19-6fadbf3b0dc0.png" width=500/>

- 첫 번째 그래프는 원본 데이터 포인트를 색으로 구분해 표시한 것이다. 
  - 이 알고리즘은 먼저 "component 1" 이라고 쓰여 있는, 분산이 가장 큰 방향을 찾는다. 
  - 이 방향 (또는 벡터) 이 데이터에서 가장 많은 정보를 담고 있는 방향이다. 
  - 다른 말로, 특성들의 상관관계가 가장 큰 방향이다. 
  - 그 다음으로, 이 알고리즘은 첫 번째 방향과 직각인 방향 중에서 가장 많은 정보를 담은 방향을 찾는다. 
  - 2차원에서는 가능한 직각 방향이 하나뿐이지만 고차원에서는 (무한히) 많은 직각 방향이 있을 수도 있다.
  - 이런 과정을 거쳐 찾은 방향을 데이터에 있는 주된 분산의 방향이라고 해서 주성분 (principal component) 라고 한다.
  - 일반적으로 원본 특성 개수 만큼의 주성분이 있다.
- 두 번째 그래프는 같은 데이터지만 주성분 1과 2를 각각 x 축과 y 축에 나란하도록 회전한 것이다. 
  - 회전하기 전에 평균을 빼서 중심을 원점에 맞췄다.
  - PCA에 의해 회전된 두 축은 연관되어 있지 않으므로 변환된 데이터의 상관관계 행렬 (coreelation matrix) 이 대각선 방향을 제외하고는 0이 된다.
- PCA 는 주성분의 일부만 남기는 차원 축소 용도로 사용할 수 있다. 이 예에서는 세 번째 그래프처럼 첫 번째 주성분만 유지하려고 한다.
  - 이렇게 하면 2차원 데이터셋이 1차원 데이터셋으로 차원이 감소한다.
  - 그러나 단순히 원본 특성 중 하나만 남기는 것은 아니다.
  - 가장 유용한 방향을 찾아서 그 방향의 성분, 즉 첫 번째 주성분을 유지하는 것이다.
- 마지막으로 데이터에 다시 평균을 더하고 반대로 회전시킨다. 이 결과가 마지막 그래프이다.
  - 이 데이터 포인트들은 원래 특성 공간에 놓여져 있지만 첫 번째 주성분의 정보만 담고 있다.
  - 이 변환은 데이터에서 노이즈를 제거하거나 주성분에서 유지되는 정보를 시각화화는 데 종종 사용된다.
- 특성의 스케일이 서로 다르면 올바른 주성분 방향을 찾을 수 없으니 PCA를 사용할 때는 표준값으로 바꿔야 한다.
- PCA는 비지도 학습이므로 회전축을  찾을 때 어떤 클래스 정보도 사용하지 않는다. 단순히 데이터에 있는 상관관계만을 고려한다.
- PCA의 단점은 그래프의 두 축을 해석하기가 쉽지 않다는 점이다.
  - 주성분은 원본 데이터에 있는 어떤 방향에 대응하는 여러 특성이 조합된 형태이다.
  - PCA 객체가 학습 될 때 (fit 메서드가 호출될 때) components_ 속성에 주성분이 저장 된다.
  - components_의 각 행은 주성분 하나씩을 나타내며 중요도에 따라 정렬되어 있다.



#### 비음수 행렬 분해 (NMF)

- NMF (non-negative matrix factorization) 는 유용한 특성을 뽑아내기 위한 또 다른 비지도 학습 알고리즘 이다.